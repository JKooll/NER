{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Ass2 [Lab9] Bi-LSTM CRF 2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9j_mZUqToSD",
        "colab_type": "text"
      },
      "source": [
        "## Download Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzvkczHo1YJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])\n",
        "\n",
        "import os\n",
        "if not os.path.exists('input'):\n",
        "    os.mkdir('input')\n",
        "# uncase datasets\n",
        "# downloadFiles([[\"input/train.csv\", \"1h_KEHz61FUJMyLkGpOzdBZhQ0-s5npou\"]]) \n",
        "# downloadFiles([[\"input/val.csv\", \"1Rga0OV-x-vCLDLIMkiW4uL5UZqmDrvJN\"]])\n",
        "# downloadFiles([[\"input/test.csv\", \"1dSnSVT6vuFxvZ8vIMQnyK_J-gXwu2nCV\"]])\n",
        "# downloadFiles([[\"input/sample_submission.csv\", \"1BJLpaxVN8XlNTnPvOtgepVxeU3FZBV4E\"]])\n",
        "\n",
        "# case datasets\n",
        "downloadFiles([[\"input/train.csv\", \"1nKzLTTgUhE6RqA7KUfpp9iOS3Yuign48\"]]) \n",
        "downloadFiles([[\"input/val.csv\", \"10C16Q_1riHyojGwFBWANMQsR2ow08aoX\"]])\n",
        "downloadFiles([[\"input/test.csv\", \"1iN_leOWkQCAZ716pO40q2KF5zwhf_MHF\"]])\n",
        "downloadFiles([[\"input/sample_submission.csv\", \"1BJLpaxVN8XlNTnPvOtgepVxeU3FZBV4E\"]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmKK7Vw9ra2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "sample_submission = pd.read_csv('input/sample_submission.csv')\n",
        "test = pd.read_csv('input/test.csv')\n",
        "train = pd.read_csv('input/train.csv')\n",
        "val = pd.read_csv('input/val.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik7zGzYP-lGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = [sentence.split() for sentence in train['Sentence']]\n",
        "target_y_train = [ner.split() for ner in train['NER']]\n",
        "val_data = [sentence.split() for sentence in val['Sentence']]\n",
        "target_y_validation = [ner.split() for ner in val['NER']]\n",
        "test_data = [sentence.split() for sentence in test['Sentence']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmL7WPDhGJGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data[1])\n",
        "print(target_y_train[1])\n",
        "print(val_data[1])\n",
        "print(target_y_validation[1])\n",
        "print(test_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U97dq_5MTvns",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcKXKATTyLf",
        "colab_type": "text"
      },
      "source": [
        "### Generate word_to_ix and tag_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdr8FeYjGMoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get features\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    \n",
        "    features = {\n",
        "        'word': word,\n",
        "        'word.lower()': word.lower(), \n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            'BOS': False\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            'EOS': False\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "    return features\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVYstplv-aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit and transform features\n",
        "class FeaturesTransformer():\n",
        "    def __init__(self, features, word_emb_model, word_emb_size):\n",
        "        self.features = features\n",
        "\n",
        "        self.word_emb_model = word_emb_model\n",
        "        self.word_emb_size = word_emb_size\n",
        "\n",
        "        self.isbos = {False: 0, True: 1}\n",
        "        self.iseos = {False: 0, True: 1}\n",
        "        \n",
        "        self.word_list = []\n",
        "        self.word_to_ix = {}\n",
        "        \n",
        "        # self.previous_word_istitle = {False: 0, True: 1}\n",
        "        # self.previous_word_isupper = {False: 0, True: 1}\n",
        "        \n",
        "        # self.after_word_istitle = {False: 0, True: 1}\n",
        "        # self.after_word_isupper = {False: 0, True: 1}\n",
        "        \n",
        "        self.word_last_three_chars = []\n",
        "        self.word_last_three_chars_to_ix = {}\n",
        "        self.word_last_two_chars = []\n",
        "        self.word_last_two_chars_to_ix = {}\n",
        "\n",
        "        self.word_isupper = {False: 0, True: 1}\n",
        "        self.word_istitle = {False: 0, True: 1}\n",
        "        self.word_isdigit = {False: 0, True: 1}\n",
        "\n",
        "    def fit(self, features):\n",
        "        for s in features:\n",
        "            for feature in s:\n",
        "                if 'word[-3:]' in self.features and feature['word[-3:]'] not in self.word_last_three_chars:\n",
        "                    self.word_last_three_chars.append(feature['word[-3:]'])\n",
        "                    self.word_last_three_chars_to_ix[feature['word[-3:]']] = len(self.word_last_three_chars_to_ix)\n",
        "\n",
        "                if 'word[-2:]' in self.features and feature['word[-2:]'] not in self.word_last_two_chars:\n",
        "                    self.word_last_two_chars.append(feature['word[-2:]'])\n",
        "                    self.word_last_two_chars_to_ix[feature['word[-2:]']] = len(self.word_last_two_chars_to_ix)\n",
        "\n",
        "    def transform(self, features):\n",
        "        \"\"\"\n",
        "        Transform features to embeddings\n",
        "        \"\"\"\n",
        "        transformed_data = []\n",
        "        for s in features:\n",
        "            temp_s = []\n",
        "            for feature in s:\n",
        "                temp_w = []\n",
        "                if 'word' in self.features:\n",
        "                    # get word2vec embedding\n",
        "                    try:\n",
        "                        temp_w.extend(self.word_emb_model.wv[feature['word']])\n",
        "                    except:\n",
        "                        temp_w.extend([0]*self.word_emb_size)\n",
        "                \n",
        "                if 'word.lower()' in self.features:\n",
        "                    # get word2vec embedding\n",
        "                    try:\n",
        "                        temp_w.extend(self.word_emb_model.wv[feature['word.lower()']])\n",
        "                    except:\n",
        "                        temp_w.extend([0]*self.word_emb_size)\n",
        "                \n",
        "                if 'word[-3:]' in self.features:\n",
        "                    # word last three chars\n",
        "                    temp_w.append(self.word_last_three_chars_to_ix[feature['word[-3:]']])\n",
        "                \n",
        "                if 'word[-2:]' in self.features:\n",
        "                    # word last two chars\n",
        "                    temp_w.append(self.word_last_two_chars_to_ix[feature['word[-2:]']])\n",
        "\n",
        "                if 'word.isupper()' in self.features:\n",
        "                    # word isupper\n",
        "                    temp_w.append(self.word_isupper[feature['word.isupper()']])\n",
        "                \n",
        "                if 'word.istitle()' in self.features:\n",
        "                    # word istitle\n",
        "                    temp_w.append(self.word_istitle[feature['word.istitle()']])\n",
        "                \n",
        "                if 'word.isdigit()' in self.features:\n",
        "                    # word isdigit\n",
        "                    temp_w.append(self.word_isdigit[feature['word.isdigit()']])\n",
        "                \n",
        "                if 'BOS' in self.features:\n",
        "                    # isbos\n",
        "                    temp_w.append(self.isbos[feature['BOS']])\n",
        "                \n",
        "                if 'EOS' in self.features:\n",
        "                    # iseos\n",
        "                    temp_w.append(self.iseos[feature['EOS']])\n",
        "                \n",
        "                temp_s.append(temp_w)\n",
        "            transformed_data.append(temp_s)\n",
        "        return transformed_data\n",
        "\n",
        "    def fit_transform(self, features):\n",
        "        self.fit(features)\n",
        "        return self.transform(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kONVmplsjMvp",
        "colab_type": "text"
      },
      "source": [
        "## Get features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kSa2B49jPad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(train_data, validation_data, test_data):\n",
        "    train_features = [sent2features(s) for s in train_data]\n",
        "    val_features = [sent2features(s) for s in validation_data]\n",
        "    test_features = [sent2features(s) for s in test_data]\n",
        "    return train_features, val_features, test_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbLunH5UT1sK",
        "colab_type": "text"
      },
      "source": [
        "### Generate Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxwwHoMlHTO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "def generate_input_embeddings(features, word_emb_model, word_emb_size, train_features, val_features, test_features):\n",
        "    # fit and transform features\n",
        "    ft = FeaturesTransformer(features, word_emb_model, word_emb_size)\n",
        "    ft.fit(train_features)\n",
        "    ft.fit(val_features)\n",
        "    ft.fit(test_features)\n",
        "    train_transformed_features = ft.transform(train_features)\n",
        "    val_transformed_features = ft.transform(val_features)\n",
        "    test_transformed_features = ft.transform(test_features)\n",
        "\n",
        "    return train_transformed_features, val_transformed_features, test_transformed_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm0PBV0OT4hv",
        "colab_type": "text"
      },
      "source": [
        "### Convert tags into idxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeYd6j_9HdWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in target_y_train+target_y_validation:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "train_output_index = to_index(target_y_train,tag_to_ix)\n",
        "val_output_index = to_index(target_y_validation,tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCFfjF09T7SP",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li2dSvqtH2TR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, tag_to_ix, embedding_dim, hidden_dim, apply_attention=False, use_bigru=False, lstm_layers=1, gru_layers=1):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.apply_attention = apply_attention\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=lstm_layers, bidirectional=True)\n",
        "        \n",
        "        if use_bigru:\n",
        "            self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers=gru_layers, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2 * self.lstm_layers, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2 * self.lstm_layers, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    \n",
        "    def cal_attention(self, hidden, input_embedding, method='dot product'):\n",
        "        if method == 'dot product':\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, input_embedding.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, input_embedding.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        elif method == 'scale dot product':\n",
        "            attn_weights = F.softmax(1/np.sqrt(hidden_size)*torch.bmm(hidden, input_embedding.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, input_embedding.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = sentence.float().view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        # calculate attention\n",
        "        if self.apply_attention:\n",
        "            lstm_out = self.cal_attention(lstm_out, embeds)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k859Byk_ILfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    score = f1_score(ground_truth, predicted, average='micro')\n",
        "    return ground_truth, predicted, accuracy, score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig8ib7v9T_-p",
        "colab_type": "text"
      },
      "source": [
        "### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxlet62eINpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def init_model(embedding_dim, hidden_dim, lstm_layers):\n",
        "    model = BiLSTM_CRF(tag_to_ix, embedding_dim, hidden_dim, lstm_layers=lstm_layers).to(device)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True, threshold=0.001)\n",
        "    return model, optimizer, scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeKg9SgrUDLx",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzbq3MxYIQnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Each epoch will take about 1-2 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "def train(model, optimizer, scheduler, train_transformed_features, train_output_index, val_transformed_features, val_output_index):\n",
        "    for epoch in range(20):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        for i, idxs in enumerate(train_transformed_features):\n",
        "            tags_index = train_output_index[i]\n",
        "\n",
        "            # Step 1. Remember that Pytorch accumulates gradients.\n",
        "            # We need to clear them out before each instance\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Step 2. Get our inputs ready for the network, that is,\n",
        "            # turn them into Tensors of word indices.\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.float).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Step 3. Run our forward pass.\n",
        "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        _, _, train_acc, train_score = cal_acc(model,train_transformed_features,train_output_index)\n",
        "        _, _, val_acc, val_score = cal_acc(model,val_transformed_features,val_output_index)\n",
        "\n",
        "        scheduler.step(val_score)\n",
        "\n",
        "        val_loss = 0\n",
        "        for i, idxs in enumerate(val_transformed_features):\n",
        "            tags_index = val_output_index[i]\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "            val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train score: %.4f | val loss: %.2f, val score: %.4f| time: %.2fs\" %(epoch+1, train_loss, train_score, val_loss, val_score, (time2-time1).total_seconds()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoUP4eKQUnTd",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQNOtwi-IW77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def test(model, val_transformed_features, val_output_index):\n",
        "    model.eval()\n",
        "\n",
        "    y_true, y_pred, _, _ = cal_acc(model, val_transformed_features, val_output_index)\n",
        "\n",
        "    def decode_output(output_list):\n",
        "        ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "        return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "    y_true_decode = decode_output(y_true)\n",
        "    y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "    print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGGIkmEy-Gi",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6FpfOtnoK1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(train_data, val_data, test_data, features, word_emb_model, word_emb_size, lstm_layers=1):\n",
        "    \"\"\"\n",
        "    train_data: Train sentence tokenize dataset. [[word1, word2, word3, ...], [], ...]\n",
        "    val_data: Validation sentence tokenize dataset.\n",
        "    test_data: Test tokenize dataset.\n",
        "    features: Features list. ['word', 'word[-3:]', 'word[-2:]', 'word.isupper()', 'word.istitle()', 'BOS', 'EOS'].\n",
        "    word_emb_model: Word embedding model name, like \"word2vec-google-news-300\".\n",
        "    word_emb_size: Word embedding model dim.\n",
        "    \"\"\"\n",
        "    # get input features\n",
        "    train_features, val_features, test_features = get_features(train_data, val_data, test_data)\n",
        "\n",
        "    # generate input embeddings\n",
        "    word_emb_model = api.load(word_emb_model)\n",
        "    train_transformed_features, val_transformed_features, test_transformed_features = generate_input_embeddings(\n",
        "        features,\n",
        "        word_emb_model, \n",
        "        word_emb_size, \n",
        "        train_features, \n",
        "        val_features, \n",
        "        test_features)\n",
        "\n",
        "    # hyperparameters    \n",
        "    embedding_dim = len(train_transformed_features[0][0])\n",
        "    hidden_dim = 50\n",
        "\n",
        "    print(\"Embedding dim: \", embedding_dim)\n",
        "\n",
        "    # get model\n",
        "    model, optimizer, scheduler = init_model(embedding_dim, hidden_dim, lstm_layers)\n",
        "\n",
        "    # train\n",
        "    train(model, optimizer, scheduler, train_transformed_features, train_output_index, val_transformed_features, val_output_index)\n",
        "\n",
        "    # test model\n",
        "    test(model,  val_transformed_features, val_output_index)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mk3GQqLZmlZ",
        "colab_type": "text"
      },
      "source": [
        "### Only word2vec embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwY1eZnJwoeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = main(train_data, val_data, test_data, ['word'], \"glove-twitter-100\", 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-tgbnsou-U_",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec + BOS + EOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG4pl9Sku_ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = main(train_data, val_data, test_data, ['word', 'BOS', 'EOS'], \"glove-twitter-100\", 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMjrxgtq7sw",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec + BOS + EOS + word[-3:] + word[-2:]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n81kZgGGqpWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = main(train_data, val_data, test_data, ['word.lower()', 'BOS', 'EOS', 'word[-3:]', 'word[-2:]'], \"glove-twitter-100\", 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDbIkp613SGW",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec + BOS + EOS + word.isupper() + word.istitle()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zjHDls71yHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = main(train_data, val_data, test_data, ['word', 'BOS', 'EOS', 'word.isupper()', 'word.istitle()'], \"glove-twitter-100\", 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkXptyue3mD7",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec + BOS + EOS + word.isupper() + word.istitle()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCJ-6PExWjlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = main(train_data, val_data, test_data, ['word.lower()', 'BOS', 'EOS', 'word.isupper()', 'word.istitle()'], \"fasttext-wiki-news-subwords-300\", 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5AgVSeXHQR",
        "colab_type": "text"
      },
      "source": [
        "### BiLSTM 2 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIt0K6C0Wd6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = main(train_data, val_data, test_data, ['word'], \"glove-twitter-25\", 25, lstm_layers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtuLeoDjzmWf",
        "colab_type": "text"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLq-jcetzs63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, './best_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4JMw87eg8-Y",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsG_RPM7nXdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('best_model.pth')\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "# get input features\n",
        "train_features, val_features, test_features = get_features(train_data, val_data, test_data)\n",
        "\n",
        "# generate input embeddings\n",
        "word_emb_model = api.load(\"glove-twitter-25\")\n",
        "_, _, test_transformed_features = generate_input_embeddings(\n",
        "    ['word'],\n",
        "    word_emb_model, \n",
        "    25, \n",
        "    train_features, \n",
        "    val_features, \n",
        "    test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQZLXoIqh6IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predicted = []\n",
        "for i,idxs in enumerate(test_transformed_features):\n",
        "    _, pred = model(torch.tensor(idxs, dtype=torch.float).to(device))\n",
        "    test_predicted += pred\n",
        "decoded_predicted = decode_output(test_predicted)\n",
        "sample_submission['Predicted'] = decoded_predicted\n",
        "sample_submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}